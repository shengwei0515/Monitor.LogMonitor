# use beats as input source
<source>
    @type beats
    tag "collect_from_beat"
</source>

<match collect_from_beat>
    @type record_reformer
    tag ${fields['tag']}
    source_type "collect_from_beat"
    remove_keys $.host, $.agent, $.fields, $.ecs, $.@metadata 
</match>

## filter for beatlogs
<filter filebeat-file>
  @type parser
  key_name message
  reserve_data true
  remove_key_name_field true
  <parse>
    @type regexp
    expression  /(?<time>[^\t]*)\t(?<level>[^\t]*)(\t\[(?<logger_name>[^\t]*)\])?\t(?<logger_caller>[^\t]*)\t(?<message>[^\t]*)(\t(?<json_data>[\w\W]*))?/
    time_format %Y-%m-%dT%H:%M:%S.%L%z
    timezone "+08:00"
    reserve_time true
  </parse>
</filter>

<match *>
@type copy
  <store>
    @log_level debug
    @type elasticsearch
    host elasticsearch
    port 9200
    type_name logs
    logstash_format true
    include_tag_key true
    utc_index true
    <buffer>
      @type file
      path /var/log/fluentd-buffers/log_monitor.buffer
      flush_mode interval
      retry_type exponential_backoff
      flush_thread_count 4
      flush_interval 5s
      retry_forever
      retry_max_interval 30
      chunk_limit_size "2M"
      queue_limit_length "8"
      overflow_action block
    </buffer>
  </store>
  <store>
    @type stdout
  </store>
</match>
